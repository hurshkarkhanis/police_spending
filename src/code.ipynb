{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv of crime data from the Marshall Project (nonprofit news org covering the U.S. criminal justice system)\n",
    "#skip 138 rows so data starts from '77 not '75\n",
    "crime = pd.read_csv('../data/crime_data.csv', skiprows=list(range(1,139)))\n",
    "\n",
    "#reading csv of city spending from the Lincoln Institute (nonprofit research for solutions to economic/social issues)\n",
    "spending_all = pd.read_csv('../data/spending_data.csv') #1977 - 2015\n",
    "\n",
    "#only taking columns that are relavant to my project and/or have sufficent usable data points (few zero, Nan values)\n",
    "spending = spending_all[['year',\n",
    "                         'City',\n",
    "                        'Libraries Expenditure', \n",
    "                        'Public Welfare Expenditure',\n",
    "                         'Police Protection Expenditure',\n",
    "                         'Corrections Expenditure']]\n",
    "\n",
    "#making a crime rate metric that add up all violent crimes (homicide, rape, robbery, aggravated assault)\n",
    "#divids by population and multiplies by 100K . this is an official metric according to\n",
    "# the office of the California Attorney General\n",
    "crime['crime_rate'] = (crime['violent_crime'] / crime['total_pop']) * 100000\n",
    "\n",
    "#getting ready to merge csv's into one by manipulating the strings in the City or department_name columns\n",
    "spending['City'] = spending['City'].str[4:-7] #remove first 4 chars and last 7\n",
    "crime['department_name'] = crime['department_name'].apply(lambda x : x.split(',', 1)[0]) #split by comma\n",
    "\n",
    "#merging two csv\n",
    "merged = crime.merge(spending, left_on=['department_name', 'year'], right_on=['City','year'])\n",
    "\n",
    "#dropping redundant and/or empty columns\n",
    "perfect = merged.drop(labels=['source', 'City', 'url', 'months_reported', 'ORI'], axis=1)\n",
    "\n",
    "#saving to csv, will be using this csv from now on\n",
    "perfect.to_csv(\"../data/master_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in master csv into variable \n",
    "data = pd.read_csv(\"../data/master_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building other CSV's that are sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by crime rate, government spending, and other factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df of top crime rates, which year they occurred, and which city they were in\n",
    "top_crime_rates = data.sort_values('crime_rate', axis =0, ascending=False)\n",
    "\n",
    "#df of top police spending, which year they occurred, and which city they were in\n",
    "top_police_spending = data.sort_values('Police Protection Expenditure', axis=0, ascending=False)\n",
    "\n",
    "#df of top corrections spending, which year they occurred, and which city they were in\n",
    "top_jail_spending = data.sort_values('Corrections Expenditure', axis=0, ascending=False)\n",
    "\n",
    "#df of top welfare spending, which year they occurred, and which city they were in\n",
    "top_welfare_spending = data.sort_values('Public Welfare Expenditure', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding \"toppers\"\n",
    "Cities that occur most often in top 50 rankings for highest crime, police spending, corrections spending, and welfare spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toppers(top):\n",
    "    #plotting the most frequent cities occuring in top rankings\n",
    "    #ex: Boston appears the most when ranking top 50 annual welfare expenditures\n",
    "    fig, ax = plt.subplots(4, 1, figsize=(50, 100))\n",
    "\n",
    "    #do I want the top 10, top 20, top 100?\n",
    "    top = top\n",
    "\n",
    "    #putting all df's into a list\n",
    "    df_names = [top_crime_rates.head(top)['department_name'],\n",
    "                top_police_spending.head(top)['department_name'], \n",
    "               top_jail_spending.head(top)['department_name'],\n",
    "               top_welfare_spending.head(top)['department_name']]\n",
    "\n",
    "\n",
    "    #setting cosmetics, bin size, colors, font, title names\n",
    "    bins_list = [top_crime_rates.head(top)['department_name'].unique().size,\n",
    "                top_police_spending.head(top)['department_name'].unique().size, \n",
    "                top_jail_spending.head(top)['department_name'].unique().size,\n",
    "                top_welfare_spending.head(top)['department_name'].unique().size]\n",
    "\n",
    "    color_list = ['red', '#003366', 'grey', '#00bfff']\n",
    "    font_size = 50\n",
    "    title_names = ['Crime Rate','Police Protection Expenditure','Corrections Expenditure', 'Public Welfare Expenditure']\n",
    "\n",
    "\n",
    "    #building 4 plots with for loop\n",
    "    for index in range(0, len(df_names)):\n",
    "        ax[index].hist(df_names[index], bins=bins_list[index], color=color_list[index], rwidth=0.9)\n",
    "        ax[index].set_title(\"# of Occurances in Top {} {} Ranking\".format(top, title_names[index]), fontsize=font_size)\n",
    "        ax[index].set_ylabel(\"# of Occurances\", fontsize = font_size)\n",
    "        ax[index].set_xlabel(\"City\", fontsize = font_size)\n",
    "        ax[index].tick_params(axis='x', labelsize=font_size, labelrotation=45)\n",
    "        ax[index].tick_params(axis='y', labelsize=font_size)\n",
    "\n",
    "\n",
    "    #plots were huge and were overlapping, needed some padding\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    plt.savefig('../slideshow_prep/overall/toppers.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used these functions in for loops make graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling data from any data frame specific to a certain city and specific years\n",
    "def pull_city(frame, place, start_year, end_year):\n",
    "    return frame.loc[(frame['department_name'] == place) & (frame['year'] >= start_year) & (frame['year'] <= end_year)]\n",
    "\n",
    "\n",
    "#pulling data from any data frame specific to a year\n",
    "def pull_year(frame, year):\n",
    "    return frame.loc[(frame['year']) == year]\n",
    "\n",
    "#fucntion to plot two data series with data on different scales\n",
    "# uses two y axes for this\n",
    "def double_graph(x, x_label, \n",
    "                y, y_color, y_marker, y_label, \n",
    "                z, z_color, z_marker, z_label,\n",
    "                city_name,\n",
    "                y_loc, z_loc,\n",
    "                fig_width=10, fig_height = 10):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    x_local = x\n",
    "    y_local = y\n",
    "    z_local = z\n",
    "    \n",
    "    ax.plot(x_local, y_local, color=y_color, marker=y_marker, label = y_label, linewidth=4, markersize=9)\n",
    "    ax.set_ylabel(y_label, color=y_color)\n",
    "    ax.set_xlabel(x_label)\n",
    "    \n",
    "    \n",
    "    ax2=ax.twinx()\n",
    "    \n",
    "    ax2.plot(x_local, z_local, color=z_color, marker=z_marker, label = z_label, linewidth=4, markersize=9)\n",
    "    ax2.set_ylabel(z_label, color=z_color)\n",
    "    \n",
    "    ax.legend(loc=y_loc)\n",
    "    ax2.legend(loc=z_loc)\n",
    "    \n",
    "    ax.set_title(\"{} {} vs {} (over time)\".format(city_name, y_label, z_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing I: welfare spending vs crime rate\n",
    "null hypothesis: \"there is no coorelation between welfare spending and crime rate\"\n",
    "\n",
    "alt. hypothesis: \"there is a coorelation between welfare spending and crime rate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z SCORE:  5.537314638239924 \n",
      " T-TEST RESULTS:  Ttest_indResult(statistic=-0.9411501334067678, pvalue=0.3467526331130981) \n",
      " MANN WHITNEY TEST RESULTS:  MannwhitneyuResult(statistic=492587.0, pvalue=0.39858068783094924)\n"
     ]
    }
   ],
   "source": [
    "#all vals are strings so remove commas and make numerical\n",
    "data['Public Welfare Expenditure'].replace(',','', regex=True, inplace=True)\n",
    "data['welfare'] = pd.to_numeric(data['Public Welfare Expenditure'])\n",
    "\n",
    "#seeing if value is above or below its own median, making a series of boolean values\n",
    "median = data['welfare'].median()\n",
    "data['welfare_above'] = data['welfare'] > median\n",
    "\n",
    "\n",
    "#LOW SPENDING\n",
    "#df of cities and years where welfare spending fell below the median of whole data set\n",
    "low_welfare_spending = data.loc[(data['welfare_above'] == False)]\n",
    "\n",
    "low_mean = low_welfare_spending['crime_rate'].mean()\n",
    "low_standard_dev = low_welfare_spending['crime_rate'].std()\n",
    "\n",
    "\n",
    "#HIGH SPENDING\n",
    "#df of cities and years where welfare spending fell above the median of whole data set\n",
    "high_welfare_spending = data.loc[(data['welfare_above'] == True)]\n",
    "\n",
    "high_mean = high_welfare_spending['crime_rate'].mean()\n",
    "high_standard_dev = high_welfare_spending['crime_rate'].std()\n",
    "\n",
    "\n",
    "#assume  null is true, finding z score\n",
    "sigma = high_standard_dev/(high_welfare_spending.size**(1/2)) \n",
    "z = (high_mean - low_mean)/(sigma)\n",
    "\n",
    "#running t test\n",
    "welfare_vs_crime_t_test = stats.ttest_ind(low_welfare_spending['crime_rate'].dropna(), high_welfare_spending['crime_rate'].dropna(),\n",
    "                        equal_var=False)\n",
    "\n",
    "#running mann whitney test\n",
    "welfare_vs_crime_mann_whitney = stats.mannwhitneyu(low_welfare_spending['crime_rate'].dropna(), high_welfare_spending['crime_rate'].dropna())\n",
    "\n",
    "\n",
    "print(\"Z SCORE: \", z, '\\n', \n",
    "      'T-TEST RESULTS: ', welfare_vs_crime_t_test, \"\\n\", \n",
    "      'MANN WHITNEY TEST RESULTS: ', welfare_vs_crime_mann_whitney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large z score but large p values (much greater than 0.05) ➡️ <br>\n",
    "\n",
    "Half of this data series was unusable, which threw off the sample sizes and also may have thrown off the mean\n",
    "\n",
    "<h3>Not able to make an accurare call as to reject to fail to reject null hypothesis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing II: corrections spending vs crime rate\n",
    "null hypothesis: \"there is no coorelation between corrections spending and crime rate\"\n",
    "\n",
    "alt. hypothesis: \"there is a coorelation between corrections spending and crime rate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z SCORE:  29.383206983299328 \n",
      " T-TEST RESULTS:  Ttest_indResult(statistic=-4.56419015625667, pvalue=5.337882950165588e-06) \n",
      " MANN WHITNEY TEST RESULTS:  MannwhitneyuResult(statistic=404164.0, pvalue=8.725127298210468e-12)\n"
     ]
    }
   ],
   "source": [
    "#all vals are strings so remove commas and make numerical\n",
    "data['Corrections Expenditure'].replace(',','', regex=True, inplace=True)\n",
    "data['corrections'] = pd.to_numeric(data['Corrections Expenditure'])\n",
    "\n",
    "#seeing if value is above or below its own median, making a series of boolean values\n",
    "median = data['corrections'].median()\n",
    "data['corrections_above'] = data['corrections'] > median\n",
    "\n",
    "\n",
    "#LOW SPENDING\n",
    "#df of cities and years where corrections spending fell below the median of whole data set\n",
    "low_corrections_spending = data.loc[(data['corrections_above'] == False)]\n",
    "\n",
    "low_mean = low_corrections_spending['crime_rate'].mean()\n",
    "low_standard_dev = low_corrections_spending['crime_rate'].std()\n",
    "\n",
    "\n",
    "#HIGH SPENDING\n",
    "#df of cities and years where corrections spending fell above the median of whole data set\n",
    "high_corrections_spending = data.loc[(data['corrections_above'] == True)]\n",
    "\n",
    "high_mean = high_corrections_spending['crime_rate'].mean()\n",
    "high_standard_dev = high_corrections_spending['crime_rate'].std()\n",
    "\n",
    "\n",
    "#assume  null is true, finding z score\n",
    "sigma = high_standard_dev/(high_corrections_spending.size**(1/2)) \n",
    "z = (high_mean - low_mean)/(sigma)\n",
    "\n",
    "#running t test\n",
    "corrections_vs_crime_t_test = stats.ttest_ind(low_corrections_spending['crime_rate'].dropna(), high_corrections_spending['crime_rate'].dropna(),\n",
    "                        equal_var=False)\n",
    "\n",
    "#running mann whitney test\n",
    "corrections_vs_crime_mann_whitney = stats.mannwhitneyu(low_corrections_spending['crime_rate'].dropna(), high_corrections_spending['crime_rate'].dropna())\n",
    "\n",
    "\n",
    "print(\"Z SCORE: \", z, '\\n', \n",
    "      'T-TEST RESULTS: ', corrections_vs_crime_t_test, \"\\n\", \n",
    "      'MANN WHITNEY TEST RESULTS: ', corrections_vs_crime_mann_whitney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large z score but large p values (much greater than 0.05) ➡️ <br>\n",
    "\n",
    "Half of this data series was unusable, which threw off the sample sizes and also may have thrown off the mean\n",
    "\n",
    "<h3>Not able to make an accurare call as to reject to fail to reject null hypothesis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was wondering...\n",
    "Why both the welfare and corrections data was so sparse? Why were only half the values there? Can I investigate more into why certain cities had welfare data and others didn't? Going into this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cities_with_perfect_welfare_reporting():\n",
    "    #toy df just to play with\n",
    "    toy = data\n",
    "\n",
    "    #new col of booleans to see if data point is zero or not\n",
    "    toy['welfare_zero'] = toy['welfare'] == 0\n",
    "\n",
    "    #filtring in only cols where welfare_zero == True\n",
    "    filter = toy[\"welfare_zero\"]==0\n",
    "    toy.where(filter, inplace = True) \n",
    "\n",
    "    #only looking at city names\n",
    "    toy['department_name'].unique()\n",
    "\n",
    "    perfect_list = []\n",
    "    for city in data['department_name'].unique():\n",
    "        if city not in toy['department_name'].unique():\n",
    "            perfect_list.append(city)\n",
    "        \n",
    "    return perfect_list\n",
    "\n",
    "def cities_with_perfect_corrections_reporting():\n",
    "    #toy df just to play with\n",
    "    toy = data\n",
    "\n",
    "    #new col of booleans to see if data point is zero or not\n",
    "    toy['corrections_zero'] = toy['Corrections Expenditure'] == 0\n",
    "\n",
    "    #filtring in only cols where welfare_zero == True\n",
    "    filter = toy[\"corrections_zero\"]==0\n",
    "    toy.where(filter, inplace = True) \n",
    "\n",
    "    #only looking at city names\n",
    "    toy['department_name'].unique()\n",
    "\n",
    "    perfect_list = []\n",
    "    for city in data['department_name'].unique():\n",
    "        if city not in toy['department_name'].unique():\n",
    "            perfect_list.append(city)\n",
    "        \n",
    "    return perfect_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every city has its struggles...\n",
    "This above functions return lists of cities who have produced one or more zeros when reporting welfare or corrections spending. Since the returned lists for both functions were empty, it means every city has failed to report welfare or corrections spending at least once. Could be an economic issue. Usually things having to do with those who are poor (those on welfare, those in jail) are underreported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing III: police spending vs crime rate\n",
    "null hypothesis: \"there is no coorelation between police spending and crime rate\"\n",
    "\n",
    "alt. hypothesis: \"there is a coorelation between police spending and crime rate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z SCORE:  19.03034219852543 \n",
      " T-TEST RESULTS:  Ttest_indResult(statistic=-15.290356764749061, pvalue=9.026722151876525e-50) \n",
      " MANN WHITNEY TEST RESULTS:  MannwhitneyuResult(statistic=309054.0, pvalue=4.72039303663214e-49)\n"
     ]
    }
   ],
   "source": [
    "#all vals are strings so remove commas and make numerical\n",
    "data['Police Protection Expenditure'].replace(',','', regex=True, inplace=True)\n",
    "data['police'] = pd.to_numeric(data['Police Protection Expenditure'])\n",
    "\n",
    "#seeing if value is above or below its own median, making a series of boolean values\n",
    "median = data['police'].median()\n",
    "data['police_above'] = data['police'] > median\n",
    "\n",
    "\n",
    "#LOW SPENDING\n",
    "#df of cities and years where police spending fell below the median of whole data set size = 1015\n",
    "low_police_spending = data.loc[(data['police_above'] == False)]\n",
    "\n",
    "low_mean = low_police_spending['crime_rate'].mean()\n",
    "low_standard_dev = low_police_spending['crime_rate'].std()\n",
    "\n",
    "#HIGH SPENDING\n",
    "#df of cities and years where police spending fell above the median of whole data set size = 1013\n",
    "high_police_spending = data.loc[(data['police_above'] == True)]\n",
    "\n",
    "high_mean = high_police_spending['crime_rate'].mean()\n",
    "high_standard_dev = high_police_spending['crime_rate'].std()\n",
    "\n",
    "#assume  null is true, finding z score\n",
    "n = (data['police_above'] == True).size/2\n",
    "sigma = high_standard_dev/(n**(1/2)) \n",
    "z = (high_mean - low_mean)/(sigma)\n",
    "\n",
    "z #19, which means p value is going to be very small, so I reject null hyopthesis\n",
    "\n",
    "#running t test\n",
    "police_vs_crime_t_test = stats.ttest_ind(low_police_spending['crime_rate'].dropna(), high_police_spending['crime_rate'].dropna(),\n",
    "                        equal_var=False)\n",
    "\n",
    "#running mann whitney test\n",
    "police_vs_crime_mann_whitney = stats.mannwhitneyu(low_police_spending['crime_rate'].dropna(), high_police_spending['crime_rate'].dropna())\n",
    "\n",
    "\n",
    "print(\"Z SCORE: \", z, '\\n', \n",
    "      'T-TEST RESULTS: ', police_vs_crime_t_test, \"\\n\", \n",
    "      'MANN WHITNEY TEST RESULTS: ', police_vs_crime_mann_whitney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large z score, very small p values ➡️ I will <b>REJECT</b> my null hypothesis<br>\n",
    "<h3>There <u>is</u> a coorelation between police spending and crime rate.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of median police spending vs crime rate for all cities (over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populates lists to plot on double graph\n",
    "#plots on double graph\n",
    "def illustrate_hypothesis():\n",
    "    list_of_police_spending = []\n",
    "    list_of_crime_rates = []\n",
    "\n",
    "    start = 1977 #first year of data\n",
    "    end = 2015 #second year of data\n",
    "\n",
    "    x = list(range(start, end+1)) # +1 because it's a range()\n",
    "\n",
    "    for year in data['year'].unique():\n",
    "        police_spending_med = pd.to_numeric(pull_year(data, year)['Police Protection Expenditure']).median()\n",
    "        crime_rate_med = pd.to_numeric(pull_year(data, year)['crime_rate']).median()\n",
    "        list_of_police_spending.append(police_spending_med)\n",
    "        list_of_crime_rates.append(crime_rate_med)\n",
    "    \n",
    "    double_graph(x, 'Year', \n",
    "                 list_of_police_spending, \"#003366\", 'h', 'Median Police Spending', \n",
    "                 list_of_crime_rates, 'red', 'h', 'Median Crime Rate',\n",
    "                 \"Sample\",\n",
    "                 'upper left', 'upper right',\n",
    "                 15, 15)\n",
    "\n",
    "    plt.savefig('../slideshow_prep/overall/illustrating_hypothesis.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
